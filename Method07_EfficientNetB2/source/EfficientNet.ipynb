{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torch import optim\n\nimport torchvision.transforms as transforms\nimport torchvision\n\nfrom fastprogress import master_bar, progress_bar\n\nfrom PIL import Image\n\nimport math\nimport torch.nn.functional as F\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom glob import glob\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport seaborn as sns","execution_count":75,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Download image and class labels\n!wget https://raw.githubusercontent.com/lukemelas/EfficientNet-PyTorch/master/examples/simple/img.jpg\n!wget https://raw.githubusercontent.com/lukemelas/EfficientNet-PyTorch/master/examples/simple/labels_map.txt\n# Get EfficientNet PyTorch\n!pip install efficientnet_pytorch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_SIZE = 224                              # Image size (224x224)\nIMAGENET_MEAN = [0.485, 0.456, 0.406]         # Mean of ImageNet dataset (used for normalization)\nIMAGENET_STD = [0.229, 0.224, 0.225]          # Std of ImageNet dataset (used for normalization)\nBATCH_SIZE = 60                           \nLEARNING_RATE = 0.001\nLEARNING_RATE_SCHEDULE_FACTOR = 0.1           # Parameter used for reducing learning rate\nLEARNING_RATE_SCHEDULE_PATIENCE = 5           # Parameter used for reducing learning rate\nMAX_EPOCHS = 90\n\nUSE_BCELOGIT = True\nUSE_WEIGHT = True\n# LOSS WEIGHT\npos_w = None\nw = None\nif USE_WEIGHT:\n    pos_w=[0.346]\n    w = [2.]","execution_count":76,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Import Path**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Define path to the data directory\ndata_dir = Path('../input/chest_xray/chest_xray')\n\n# Path to train directory (Fancy pathlib...no more os.path!!)\ntrain_dir = data_dir / 'train'\n\n# Path to validation directory\nval_dir = data_dir / 'val'\n\n# Path to test directory\ntest_dir = data_dir / 'test'","execution_count":77,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load img**"},{"metadata":{},"cell_type":"markdown","source":"For training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the path to the normal and pneumonia sub-directories\nnormal_cases_dir = train_dir / 'NORMAL'\npneumonia_cases_dir = train_dir / 'PNEUMONIA'\n\n# Get the list of all the images\nnormal_cases = normal_cases_dir.glob('*.jpeg')\npneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n\n# An empty list. We will insert the data into this list in (img_path, label) format\ntrain_data = []\n\n# Go through all the normal cases. The label for these cases will be 0\nfor img in normal_cases:\n    train_data.append((img,0))\n    train_data.append((img,0))\n# Go through all the pneumonia cases. The label for these cases will be 1\nfor img in pneumonia_cases:\n    train_data.append((img, 1))\n\n# Get a pandas dataframe from the data we have in our list \ntrain_data = pd.DataFrame(train_data, columns=['path', 'label'],index=None)\n\n# Shuffle the data \ntrain_data = train_data.sample(frac=1.).reset_index(drop=True)\n\n# How the dataframe looks like?\ntrain_data.head()","execution_count":78,"outputs":[{"output_type":"execute_result","execution_count":78,"data":{"text/plain":"                                                path  label\n0  ../input/chest_xray/chest_xray/train/NORMAL/IM...      0\n1  ../input/chest_xray/chest_xray/train/PNEUMONIA...      1\n2  ../input/chest_xray/chest_xray/train/PNEUMONIA...      1\n3  ../input/chest_xray/chest_xray/train/PNEUMONIA...      1\n4  ../input/chest_xray/chest_xray/train/NORMAL/IM...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>../input/chest_xray/chest_xray/train/NORMAL/IM...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>../input/chest_xray/chest_xray/train/PNEUMONIA...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>../input/chest_xray/chest_xray/train/PNEUMONIA...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>../input/chest_xray/chest_xray/train/PNEUMONIA...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>../input/chest_xray/chest_xray/train/NORMAL/IM...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"For valid"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the path to the normal and pneumonia sub-directories\nnormal_cases_dir = val_dir / 'NORMAL'\npneumonia_cases_dir = val_dir / 'PNEUMONIA'\n\n# Get the list of all the images\nnormal_cases = normal_cases_dir.glob('*.jpeg')\npneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n\n# An empty list. We will insert the data into this list in (img_path, label) format\nvalid_data = []\n\n# Go through all the normal cases. The label for these cases will be 0\nfor img in normal_cases:\n    valid_data.append((img, 0))\n# Go through all the pneumonia cases. The label for these cases will be 1\nfor img in pneumonia_cases:\n    valid_data.append((img, 1))\n\n# Get a pandas dataframe from the data we have in our list \nvalid_data = pd.DataFrame(valid_data, columns=['path', 'label'],index=None)\n\n# Shuffle the data \nvalid_data = valid_data.sample(frac=1.).reset_index(drop=True)\n\n# How the dataframe looks like?\nvalid_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the path to the normal and pneumonia sub-directories\nnormal_cases_dir = test_dir / 'NORMAL'\npneumonia_cases_dir = test_dir / 'PNEUMONIA'\n\n# Get the list of all the images\nnormal_cases = normal_cases_dir.glob('*.jpeg')\npneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n\n# An empty list. We will insert the data into this list in (img_path, label) format\ntest_data = []\n\n# Go through all the normal cases. The label for these cases will be 0\nfor img in normal_cases:\n    test_data.append((img, 0))\n# Go through all the pneumonia cases. The label for these cases will be 1\nfor img in pneumonia_cases:\n    test_data.append((img, 1))\n\n# Get a pandas dataframe from the data we have in our list \ntest_data = pd.DataFrame(test_data, columns=['path', 'label'],index=None)\n\n# Shuffle the data \ntest_data = test_data.sample(frac=1.).reset_index(drop=True)\n\n# How the dataframe looks like?\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **Show Figue of Data**"},{"metadata":{},"cell_type":"markdown","source":"Training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the counts for each class\ncases_count = train_data['label'].value_counts()\nprint(cases_count)\n\n# Plot the results \nplt.figure(figsize=(10,8))\nsns.barplot(x=cases_count.index, y= cases_count.values)\nplt.title('Number of cases', fontsize=14)\nplt.xlabel('Case type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(range(len(cases_count.index)), ['Normal(0)', 'Pneumonia(1)'])\nplt.show()","execution_count":79,"outputs":[{"output_type":"stream","text":"0    4023\n1    3875\nName: label, dtype: int64\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x576 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"Dataset is very imbalance, so we need Aug, Overstampling or Understampling"},{"metadata":{},"cell_type":"markdown","source":"Valid dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the counts for each class\ncases_count = valid_data['label'].value_counts()\nprint(cases_count)\n\n# Plot the results \nplt.figure(figsize=(10,8))\nsns.barplot(x=cases_count.index, y= cases_count.values)\nplt.title('Number of cases', fontsize=14)\nplt.xlabel('Case type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(range(len(cases_count.index)), ['Normal(0)', 'Pneumonia(1)'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the counts for each class\ncases_count = test_data['label'].value_counts()\nprint(cases_count)\n\n# Plot the results \nplt.figure(figsize=(10,8))\nsns.barplot(x=cases_count.index, y= cases_count.values)\nplt.title('Number of cases', fontsize=14)\nplt.xlabel('Case type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(range(len(cases_count.index)), ['Normal(0)', 'Pneumonia(1)'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create FudusDataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install imgaug","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imgaug import augmenters as iaa \nfrom PIL import Image\nfrom matplotlib import cm\n\nclass FundusDataset(Dataset):\n    \n    def __init__(self, dataframe, image_size, normalization):\n        \"\"\"\n        Init Dataset\n        \n        Parameters\n        ----------\n        dataframe: pandas.DataFrame\n            dataframe contains all information of images\n        image_size: int\n            image size to rescale\n        normalization: bool\n            whether applying normalization with mean and std from ImageNet or not\n        \"\"\"\n        self.image_paths = [] # List of image paths\n        self.image_labels = [] # List of image labels\n        self.prev_path = None\n        self.prev_prev_path = None\n        \n        # Define list of image transformations\n        image_transformation = [\n            transforms.Resize((image_size, image_size)),\n            transforms.ToTensor()\n        ]\n        \n        if normalization:\n            # Normalization with mean and std from ImageNet\n            image_transformation.append(transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD))\n        \n        self.image_transformation = transforms.Compose(image_transformation)\n        \n        # Get all image paths and image labels from dataframe\n        for index, row in dataframe.iterrows():\n            self.image_paths.append(row.path)\n            self.image_labels.append(row.label)\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        \"\"\"\n        Read image at index and convert to torch Tensor\n        \"\"\"\n        # Read image\n        image_path = self.image_paths[index]\n        image_data = Image.open(image_path).convert(\"RGB\") # Convert image to RGB channels\n        \n        # TODO: Image augmentation code would be placed here\n        \n        # Resize and convert image to torch tensor \n        image_data = self.image_transformation(image_data)\n        \n        return image_data, torch.tensor([self.image_labels[index]*1.0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = FundusDataset(train_data, IMAGE_SIZE, True)\nvalid_dataset = FundusDataset(valid_data, IMAGE_SIZE, True)\ntest_dataset = FundusDataset(test_data, IMAGE_SIZE, True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"for img, label in test_dataset:\n    print(img.size())\n    if label == 0:\n        print(0)\n    else:\n        print(1)\n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create Dataloader**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"test dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"for data, label in valid_dataloader:\n    print(data.size())\n    print(label.size())\n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check GPU**"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Define Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Efficient net\nfrom efficientnet_pytorch import EfficientNet\nmodel_name = 'efficientnet-b2'\n\nclass EffNet(nn.Module):\n    def __init__(self, num_classes, is_trained=True):\n        \"\"\"\n        Init model architecture\n        \n        Parameters\n        ----------\n        num_classes: int\n            number of classes\n        is_trained: bool\n            whether using pretrained model from ImageNet or not\n        \"\"\"\n        super().__init__()\n        \n        # Load the pretrained \n        self.net = EfficientNet.from_pretrained(model_name)\n\n        \n        # Get the input dimension of last layer\n        kernel_count = self.net._fc.in_features\n        \n        # Replace last layer with new layer that have num_classes nodes, after that apply Sigmoid to the output\n        if not USE_BCELOGIT:\n            self.net._fc = nn.Sequential(nn.Linear(kernel_count, num_classes)\n                                       ,nn.Sigmoid()\n                                       )\n        else:\n            self.net._fc = nn.Sequential(nn.Linear(kernel_count, num_classes)\n                                       #,nn.Sigmoid()\n                                       )\n        \n    def forward(self, inputs):\n        \"\"\"\n        Forward the netword with the inputs\n        \"\"\"\n        output = self.net(inputs)\n        return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ** Create Model & count params**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = EffNet(num_classes=1).to(device)\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(p.numel() for p in model.parameters() if p.requires_grad)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define loss function, optimizer, and learning rate scheduler"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss function\nif pos_w is not None:\n    pos_w = torch.tensor(pos_w, device=device)\nif w is not None:\n    w = torch.tensor(w, device=device)\n\nif USE_BCELOGIT:\n    loss_criteria = nn.BCEWithLogitsLoss(weight=w, pos_weight=pos_w)\nelse:\n    loss_criteria = nn.BCELoss(weight=w)\nif device == 'cuda':\n    loss_criteria.cuda()\n\n# Adam optimizer\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-5)\n\n# Learning rate will be reduced automatically during training\nlr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = LEARNING_RATE_SCHEDULE_FACTOR, patience = LEARNING_RATE_SCHEDULE_PATIENCE, mode = 'max', verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compute F1 Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"def multi_label_f1(y_gt, y_pred):\n    \"\"\" Calculate F1 for each class\n\n    Parameters\n    ----------\n    y_gt: torch.Tensor\n        groundtruth\n    y_pred: torch.Tensor\n        prediction\n\n    Returns\n    -------\n    list\n        F1 of each class\n    \"\"\"\n    f1_out = []\n    gt_np = y_gt.to(\"cpu\").numpy()\n    pred_np = (y_pred.to(\"cpu\").numpy() > 0.5) * 1.0\n    assert gt_np.shape == pred_np.shape, \"y_gt and y_pred should have the same size\"\n    for i in range(gt_np.shape[1]):\n        f1_out.append(f1_score(gt_np[:, i], pred_np[:, i]))\n    return f1_out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training each epoc"},{"metadata":{"trusted":true},"cell_type":"code","source":"def epoch_training(epoch, model, train_dataloader, device, loss_criteria, optimizer, mb):\n    \"\"\"\n    Epoch training\n\n    Paramteters\n    -----------\n    epoch: int\n      epoch number\n    model: torch Module\n      model to train\n    train_dataloader: Dataset\n      data loader for training\n    device: str\n      \"cpu\" or \"cuda\"\n    loss_criteria: loss function\n      loss function used for training\n    optimizer: torch optimizer\n      optimizer used for training\n    mb: master bar of fastprogress\n      progress to log\n\n    Returns\n    -------\n    float\n      training loss\n    \"\"\"\n    # Switch model to training mode\n    model.train()\n    training_loss = 0 # Storing sum of training losses\n   \n    # For each batch\n    for batch, (images, labels) in enumerate(progress_bar(train_dataloader, parent=mb)):\n        \n        # Move X, Y  to device (GPU)\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Clear previous gradient\n        optimizer.zero_grad()\n\n        # Feed forward the model\n        pred = model(images)\n        loss = loss_criteria(pred, labels)\n\n        # Back propagation\n        loss.backward()\n\n        # Update parameters\n        optimizer.step()\n\n        # Update training loss after each batch\n        training_loss += loss.item()\n\n        mb.child.comment = f'Training loss {training_loss/(batch+1)}'\n\n    del images, labels, loss\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n\n    # return training loss\n    return training_loss/len(train_dataloader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluating(epoch, model, val_loader, device, loss_criteria, mb):\n    \"\"\"\n    Validate model on validation dataset\n    \n    Parameters\n    ----------\n    epoch: int\n        epoch number\n    model: torch Module\n        model used for validation\n    val_loader: Dataset\n        data loader of validation set\n    device: str\n        \"cuda\" or \"cpu\"\n    loss_criteria: loss function\n      loss function used for training\n    mb: master bar of fastprogress\n      progress to log\n  \n    Returns\n    -------\n    float\n        loss on validation set\n    float\n        metric score on validation set\n    \"\"\"\n\n    # Switch model to evaluation mode\n    model.eval()\n\n    val_loss = 0                                   # Total loss of model on validation set\n    out_pred = torch.FloatTensor().to(device)      # Tensor stores prediction values\n    out_gt = torch.FloatTensor().to(device)        # Tensor stores groundtruth values\n\n    with torch.no_grad(): # Turn off gradient\n        # For each batch\n        for step, (images, labels) in enumerate(progress_bar(val_loader, parent=mb)):\n            # Move images, labels to device (GPU)\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # Update groundtruth values\n            out_gt = torch.cat((out_gt,  labels), 0)\n\n            # Feed forward the model\n            ps = model(images)\n            loss = loss_criteria(ps, labels)\n\n            # Update prediction values\n            out_pred = torch.cat((out_pred, ps), 0)\n\n            # Update validation loss after each batch\n            val_loss += loss\n            mb.child.comment = f'Validation loss {val_loss/(step+1)}'\n\n    # Clear memory\n    del images, labels, loss\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n    # return validation loss, and metric score\n    return val_loss/len(val_loader), np.array(multi_label_f1(out_gt, out_pred)).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fully Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best F1 value during training\nbest_score = 0\nmodel_path = \"efficientB4.pth\"\ntraining_losses = []\nvalidation_losses = []\nvalidation_score = []\n\n\n# Config progress bar\nmb = master_bar(range(MAX_EPOCHS))\nmb.names = ['Training loss', 'Validation loss', 'Validation F1']\nx = []\n\n# Training each epoch\nfor epoch in mb:\n    mb.first_bar.comment = f'Best F1 score: {best_score}'\n    x.append(epoch)\n\n    # Training\n    train_loss = epoch_training(epoch, model, train_dataloader, device, loss_criteria, optimizer, mb)\n    mb.write('Finish training epoch {} with loss {:.4f}'.format(epoch, train_loss))\n    training_losses.append(train_loss)\n\n    # Evaluating\n    val_loss, new_score = evaluating(epoch, model, valid_dataloader, device, loss_criteria, mb)\n    mb.write('Finish validation epoch {} with loss {:.4f} and score {:.4f}'.format(epoch, val_loss, new_score))\n    validation_losses.append(val_loss)\n    validation_score.append(new_score)\n\n    # Update learning rate\n    lr_scheduler.step(new_score)\n\n    # Update training chart\n    mb.update_graph([[x, training_losses], [x, validation_losses], [x, validation_score]], [0,MAX_EPOCHS], [0,1])\n\n    # Save model\n    if best_score < new_score:\n        mb.write(f\"Improve F1 from {best_score} to {new_score}\")\n        best_score = new_score\n\n        # Saving model: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n        torch.save(model.state_dict(), model_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing result"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(image_data, model, device, threshold):\n    \"\"\" Predict image\n    \n    Parameters\n    ----------\n    image_path: str\n        image to predict\n    model: nn.Module\n        model used to predict\n    device: str\n        'cpu' or 'cuda'\n        \n    Returns\n    -------\n    str\n        list of label indices\n    \"\"\"\n    with torch.no_grad():\n        ps = model(image_data.unsqueeze(0).to(device))\n        ps = ps[0]\n        if ps[0].item() >= threshold: \n            return 1.\n    return 0.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_state_dict(torch.load(model_path))\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxScore = 0.\nthresholdPerfect = 0.\nthreshold = [3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0]\nfor element in threshold:\n    count_truth = 0\n    count_img = 0\n    for img, label in test_dataset:\n        if predict(img, model, device, element) == label[0]:\n            count_truth+=1\n        count_img+=1\n    if (count_truth/count_img) > maxScore:\n        maxScore = count_truth/count_img\n        thresholdPerfect = element\n    print(f'{element} - {count_truth/count_img}')\n\n\nprint(\"Score = \",maxScore)\nprint(\"threshold = \", thresholdPerfect)","execution_count":74,"outputs":[{"output_type":"stream","text":"3.0 - 0.9278846153846154\n3.1 - 0.9278846153846154\n3.2 - 0.9262820512820513\n3.3 - 0.9262820512820513\n3.4 - 0.9246794871794872\n3.5 - 0.9262820512820513\n3.6 - 0.9262820512820513\n3.7 - 0.9262820512820513\n3.8 - 0.9262820512820513\n3.9 - 0.9246794871794872\n4.0 - 0.9230769230769231\nScore =  0.9278846153846154\nthreshold =  3.0\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}